<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DSC80 Note (Simplified Version) | Yue Yang</title> <meta name="author" content="Yue Yang"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yy-gx.github.io//blog/2019/DSC80/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yue </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Yue_Yang.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">DSC80 Note (Simplified Version)</h1> <p class="post-meta">October 11, 2019• Y. Yang</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/ucsd-courses"> <i class="fas fa-hashtag fa-sm"></i> UCSD-Courses</a>   </p> </header> <article class="post-content"> <blockquote> <p>This is the summary note of DSC80(Fall 2019) in UCSD. Part of this note is derived from the course jupyter notebook. This is the link of this course’s main page: https://sites.google.com/eng.ucsd.edu/dsc-80-fall-2019</p> </blockquote> <h1 id="notes">Notes</h1> <h2 id="lecture-1--2">Lecture 1 &amp; 2</h2> <p>axis = 0 和 axis = 1的区别记忆:</p> <p>一般方法的默认都是axis=0</p> <p>然后axis = 1就是方法结束后行数是不会变的, axis = 0方法结束之后列数是不会变的</p> <h2 id="lecture-3-messay-data">Lecture 3. Messay data</h2> <p><img src="https://i.loli.net/2019/11/05/IPwq2LZYADdgB9M.png" alt="image.png"></p> <ul> <li> <p>quantiattive data math model on it</p> </li> <li> <p>Categorical data</p> <ul> <li>Ordinal: can’t do math</li> <li>Nominal: just labels, no order</li> </ul> </li> </ul> <hr> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Danger! 就是强行的数据类型的转换，然后有error的地方就设置为NaN
df['DSC 80 Final Grade'] = pd.to_numeric(df['DSC 80 Final Grade'], errors='coerce')
</code></pre></div></div> <hr> <ul> <li>The result of <em>any</em> comparison (=,!=,&lt;,&gt;) with <code class="language-plaintext highlighter-rouge">NaN</code> is <code class="language-plaintext highlighter-rouge">False</code>. <ul> <li>Use functions for checking null: <code class="language-plaintext highlighter-rouge">np.isnan</code>, <code class="language-plaintext highlighter-rouge">np.isnull</code> </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">NaN</code> is of float-type.</li> <li>Be careful of Pandas type-coercian with <code class="language-plaintext highlighter-rouge">NaN</code>!</li> </ul> <h2 id="lecture-4-testing-hypotheses">Lecture 4. Testing Hypotheses</h2> <h3 id="1-test-statistic">1. test statistic</h3> <p>Measure information that answers the questions we’re trying to answer.</p> <ul> <li> <strong>Difference in means</strong>: is similar to the absolute difference in means except that the difference is “directional”. When you use the difference in means as a test statistic in a permutation test, you must specify the direction of your alternative hypothesis.</li> <li> <strong>Absolute difference in means</strong>: compares a measure of center of the two distributions</li> <li> <strong>KS</strong>: compares the overall shape of the two distributions. Have lower p-values. In cases where you want to test whether two quantitative distributions are different, the ks statistic is often more preferable for this reason.</li> </ul> <h3 id="2-null-and-alternative">2. Null and alternative</h3> <ul> <li>The method only works if we can simulate data under one of the hypotheses.</li> <li> <strong>Null hypothesis</strong>: <br>Often (but not always) the null hypothesis states there is no association or difference between variables or subpopulations. Null hyp必然是俩sample相等,也就是<strong>A = B</strong>这样的. <ul> <li>A well defined probability model about how the data were generated</li> <li>We can simulate data under the assumptions of this model – “under the null hypothesis”</li> </ul> </li> <li> <strong>Alternative hypothesis</strong>: <br>Hypothesis that sample observations are influenced by some non-random cause (unlike Null Hypothesis). Altern hyp必然是俩sample不等的样子,至于是&gt; / &lt; / !=, 取决于你想要test什么.如果你想要测试<strong>A &gt; B</strong>,那altern hy就是<strong>A &gt; B</strong>. <ul> <li>A different view about the origin of the data</li> </ul> </li> </ul> <h3 id="3-p-value-observed-significance-level">3. P-value (observed significance level)</h3> <p>The P-value is the chance, under the null hypothesis, that the test statistic is equal to the value that was observed in the data or is even further in the direction of the alternative.</p> <ul> <li> <p><strong>“Inconsistent”</strong>: The test statistic is in the tail of the empirical distribution under the null hypothesis</p> </li> <li> <strong>“In the tail,” first convention</strong>: <ul> <li>The area in the tail is less than 5%</li> <li>The result is “statistically significant”</li> </ul> </li> <li> <strong>“In the tail,” second convention</strong>: <ul> <li>The area in the tail is less than 1%</li> <li>The result is “highly statistically significant”</li> </ul> </li> </ul> <h2 id="lecture-5-data-granularity">Lecture 5. Data Granularity</h2> <p><img src="https://i.loli.net/2019/11/05/Cask7V6oLByvqpm.png" alt="image.png"></p> <ul> <li> <strong>split</strong> breaks up and groups a <code class="language-plaintext highlighter-rouge">DataFrame</code> depending on the value of the specified key.</li> <li> <strong>apply</strong> computes a function (e.g. aggregate, transformation, or filtering) within the individual groups.</li> <li> <strong>combine</strong> merges the results of these operations into an output array.</li> </ul> <hr> <ul> <li> <code class="language-plaintext highlighter-rouge">dataframe.groupby(key)</code> returns a <code class="language-plaintext highlighter-rouge">DataFrameGroupBy</code> object.</li> <li> <code class="language-plaintext highlighter-rouge">.group</code> is a dictionary of grouping keys and the corresponding dataframe</li> <li> <code class="language-plaintext highlighter-rouge">.get_group(key)</code> method returns a dataframe corresponding to the given key</li> </ul> <hr> <ul> <li>We will commonly combine <code class="language-plaintext highlighter-rouge">groupby</code> with column selection: <ul> <li>e.g., <code class="language-plaintext highlighter-rouge">df.groupby("Region")["Sales"]</code> </li> </ul> </li> <li>Then add an aggregate calculation on that column:</li> </ul> <hr> <p>aggregate:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># aggregate

def avg_str_len(series):
    return series.str.len().mean()  # purpose?

res = (
    people
        .groupby(["Color", "Gender"])
        .aggregate({"Name": avg_str_len, "Number": np.mean})  #就是把方法用到那个列上面
)
</code></pre></div></div> <hr> <p>pivot / pivot_table</p> <h2 id="lecture-6-combining-data">Lecture 6. combining data</h2> <h3 id="1-pdconcat">1. pd.concat()</h3> <p>上下合并 <code class="language-plaintext highlighter-rouge">pd.concat([class_1,class_2], ignore_index=True)</code></p> <h3 id="2-merge">2. merge()</h3> <p><code class="language-plaintext highlighter-rouge">df3 = df1.merge(df2)</code> 默认采用inner merge</p> <p>merge的方法选择使用<code class="language-plaintext highlighter-rouge">how</code>属性: <code class="language-plaintext highlighter-rouge">df3_outer = pd.merge(df1, df2, how = "outer")</code> 总共4种:</p> <ul> <li>inner</li> <li>outer</li> <li>left</li> <li>right</li> </ul> <p>若俩df有好几个相同的列,那么要用<code class="language-plaintext highlighter-rouge">on</code>来表示根据哪一列来merge</p> <p>可以加<code class="language-plaintext highlighter-rouge">suffix = ()</code>来区分那些没选中的相同列</p> <h3 id="3-join">3. join()</h3> <p><code class="language-plaintext highlighter-rouge">df1.join(df2)</code> 默认采用left join</p> <p>是根据df2的索引来join的</p> <h2 id="lecture-7-permutation-test">Lecture 7. permutation test</h2> <h3 id="1-testing-through-simulation">1. Testing through simulation</h3> <ul> <li> <strong>Statistic</strong>: Difference between means.</li> <li> <strong>Null hypothesis</strong>: The two groups are sampled from the same distribution.</li> <li>Note that the null hypothesis doesn’t say <em>what</em> the distribution is.我感觉这就是判断用hyp还是per的关键处,就是simul的时候,能不能直接根据distr来生成samples <ul> <li>Different from jury panel example, fair coin example, etc.</li> <li>We can’t draw directly from the distribution!</li> </ul> </li> <li>We have to do something a bit more clever.</li> </ul> <h3 id="2-permutation-tests">2. Permutation tests</h3> <ul> <li>Perhaps the difference in means we saw is due to random chance in assignment.</li> <li> <strong>Permutation test</strong>: Shuffle the group labels a bunch of times; how often do we see a statistic this extreme?</li> <li>Randomly permuting labels is equivalent to randomly assigning birth weights to groups (without changing group sizes)</li> <li>If we <em>rarely</em> see something this extreme, then the null hypothesis doesn’t look likely.</li> </ul> <p><strong>It doesn’t matter which column we shuffle!</strong></p> <h3 id="3-permutation-test-summary">3. Permutation Test Summary</h3> <ul> <li>A permutation test <em>is a hypothesis test</em> <ul> <li>Null hypothesis states “two observations come from same distribution”. (空假设就是俩分布相等)</li> <li>Simulate null hypothesis using <em>permutations</em>.</li> </ul> </li> <li>Don’t have to know <em>what</em> the distributions are!</li> <li>When test-statistic is is difference-in-means &lt;=&gt; two sample t-test</li> <li>Permutation tests are more generally applicable.</li> </ul> <h3 id="4-permutation使用总结">4. Permutation使用总结</h3> <p>凡是使用到permutation其实就是会出现俩variables.</p> <ul> <li>一个var必然是categorical的,就是按照这个来归类的(设该var为G).</li> <li>另外一个var有两种情况: <ul> <li>一种是这个var是quantitative的,这样就直接使用groupby的方法.然后根据统计量的选择进行不同的操作. <ul> <li>difference in mean: groupby()之后用mean()+diff()+iloc就行了: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  smoking_and_birthweight.groupby('Maternal Smoker').mean().diff().iloc[-1,0]
</code></pre></div> </div> </li> <li>absolute difference in mean: 同上,只不过diff之后abs一下</li> <li>KS: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  grps = shuffled.groupby('group')['data']
  ks = ks_2samp(grps.get_group('A'), grps.get_group('B')).statistic
</code></pre></div> </div> </li> </ul> </li> <li>一种是这个var是categorical的,这样就要使用pivot_table(aggfunc=’size’) + 归一 + TVD(就是两列1范数的差除以2): <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>      # compute the tvd
      shuffled = (
          shuffled
          .pivot_table(index='is_null', columns='gender', aggfunc='size')
          .apply(lambda x:x / x.sum(), axis=1)
      )        
      tvd = shuffled.diff().iloc[-1].abs().sum() / 2
</code></pre></div> </div> <p><strong>absolute difference in mean 是求平均值的差的绝对值. TVD是直接做差取绝对值再求和,另外TVD只能用在categorical的情况下</strong></p> </li> </ul> </li> </ul> <p><strong>missing 的type判断就是permutation的应用而已,无非是G的分类是按照isnull来的.</strong></p> <h2 id="lecture-8-missingness-mechansisms">Lecture 8. Missingness Mechansisms</h2> <h3 id="1-understanding-how-data-is-absent">1. Understanding how data is absent</h3> <h4 id="missing-by-design-md">Missing by Design (MD)</h4> <ul> <li>The field being absent is deterministic.</li> <li>A function of the rows of the dataset that can: <ul> <li>exactly predict when a colum will be null,</li> <li>with only knowledge of the other columns.</li> </ul> </li> </ul> <h4 id="unconditionally-ignorable-mcar">Unconditionally ignorable (MCAR)</h4> <ul> <li>The missing value isn’t associated to the (actual, unreported) value itself, nor the values in any other fields.</li> <li>The missingness is unconditionally uniform across rows.</li> <li>Example 1: follow-up survey questions on a random sample of respondents.</li> <li>Example 2: Water damage to paper forms prior to entry (assuming shuffled forms).</li> </ul> <h4 id="conditionally-ignorable-mar">Conditionally Ignorable (MAR)</h4> <ul> <li>A missing value may depend only on values of other fields, but not its own.</li> <li>The missingness is uniform across rows, perhaps conditional on another column.</li> <li>Example 1: For really sick patients, clinicians may not draw blood for routine labs.</li> <li>Example 2: People working in a Service Industry are less likely to report their income.</li> </ul> <h4 id="non-ignorable-nmar">Non-Ignorable (NMAR)</h4> <p><strong>注意! NMAR是指缺失的原因跟这些缺失的值有关,也就是某var缺失值,看跟这个var缺失的那些值有没有关系.而非指跟其他没写上去的var有没有关系!</strong></p> <ul> <li>A missing value depends on the value of the (actual, unreported) variable that’s missing.</li> <li>Example 1: people with high income are less likely to report income.</li> <li>Example 2: a person doesn’t take a drug test because they took drugs the day before.</li> </ul> <h3 id="2-simulate的逻辑">2. simulate的逻辑</h3> <p>就是permutation的应用.这种都是俩列,一列是含有缺失值的(记为M),一列是要去看有木有MAR的(记为D)</p> <p>null hype就是M中missing的data对应到D中的那些data的distr和M中not missing的data对应到D中的那些data的distr是一样的.</p> <p>后面步骤就是常规的那些了.</p> <h2 id="lecture-9-data-imputation">Lecture 9. Data Imputation</h2> <h3 id="1-ways-of-dealing-with-missing-data">1. ways of dealing with missing data</h3> <h4 id="1-dropna就是删去一整行数据">1. dropna(就是删去一整行数据)</h4> <p>Only MCAR is unbias</p> <p>For others, it’s bad.</p> <h4 id="2-imputation">2. imputation</h4> <ul> <li>imputation with a single value: mean, median, mode</li> <li>imputation with a single value, using a model: regression, kNN</li> <li>(probabilistic) imputation by drawing from a distribution <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>heights_mar_cat_mfilled = heights_mar_cat.fillna(heights_mar_cat.child.mean())
</code></pre></div> </div> </li> </ul> <h5 id="1-imputation-with-single-value">1. imputation with single value</h5> <ol> <li>imputation with mean on whole <ul> <li>Imputing a missing value with the mean:</li> </ul> <ul> <li>preserves the mean of the observed data</li> <li>decreases the variance</li> <li>biases the means across groups when not <em>MCAR</em> <br>即总体无偏但是如果不是MCAR就每一组有偏</li> </ul> </li> <li>imputation with mean in each group (group-wise mean imputation) <br>When using MAR data, mean imputation will cause bias in each group. <br>But it is MCAR in each group of MAR, so can do mean imputation in each group.</li> </ol> <h5 id="2-imputation-with-distribution">2. imputation with distribution</h5> <ul> <li>We can <em>probalistically</em> impute missing data from a distribution. <ul> <li>Fill in missing data by drawing from the distribution of <em>non-missing</em> data.</li> </ul> </li> </ul> <h5 id="3-multiple-imputation">3. Multiple imputation</h5> <p>说白了,就是n多个填充列取平均得到的东西</p> <ul> <li> <strong>Imputation</strong>: Impute a missing data multiple times (m times)</li> <li> <strong>Analysis</strong>: Analyze each complete dataset separetly (m sets)</li> <li> <strong>Pooling</strong>: Combine multiple analysis result. M estimates result in the final estimate.</li> </ul> <h2 id="lecture-10-collecting-data">Lecture 10. Collecting Data</h2> <h3 id="1-request-response-model">1. Request-response model</h3> <ul> <li>The <em>request</em> is made by the client: <ul> <li>A computer web browser is the <code class="language-plaintext highlighter-rouge">client</code> to HTTP</li> <li>“Client requests to view a video”</li> </ul> </li> <li>The server returns a <em>response</em> to the request. <ul> <li>Youtube is a <code class="language-plaintext highlighter-rouge">server</code> that is sitting somewhere else.</li> <li>“The content is served in a response”</li> </ul> </li> </ul> <h3 id="2-request-methods">2. Request methods</h3> <ul> <li> <p>GET: is used to request data from a specified resource.</p> </li> <li> <p>HEAD: is almost identical to GET, but without the response body. HEAD requests are useful for checking what a GET request will return before actually making a GET request - like before downloading a large file or response body.</p> </li> <li> <p>DELETE: deletes the specified resource.</p> </li> <li> <p>POST: is used to send data to the server, for example, customer information, file upload, etc. using HTML forms.</p> </li> </ul> <h3 id="3-data-formats">3. data formats</h3> <ul> <li>GET: html(websites) / json(API)</li> <li>POST: json(always)</li> </ul> <h3 id="4-get-html">4. Get HTML</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>url = ''
r = request.get(url)
urlText = t.text()
&lt;!-- urlText is the raw html  --&gt;
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!-- the following is parsing --&gt;
&lt;!-- If you want scarpe tabular data --&gt;
nba = pd.read_html('https://www.basketball-reference.com/leagues/NBA_2017_per_game.html')
&lt;!-- Else --&gt;
soup = bs4.BeautifulSoup(urlText, 'html.parser')
</code></pre></div></div> <h3 id="5-depth-first-search-dfs">5. Depth First Search (DFS)</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def dfs(visit, elt):
    if elt not in visit:
        visit.append(elt)
        print(elt.name)
        for e in elt.children:
            if not isinstance(e, 'str):
                dfs(e)
    return visit
</code></pre></div></div> <h2 id="11-parsing-nested-data">11. Parsing nested data</h2> <h3 id="1-useful-methods-in-bs4">1. useful methods in bs4</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">find(attrs={'': ''})</code> 找第一个</li> <li> <code class="language-plaintext highlighter-rouge">find_all()</code> 找所有</li> <li><code class="language-plaintext highlighter-rouge">get('href')</code></li> <li> <code class="language-plaintext highlighter-rouge">.text</code> 获取tag的content</li> </ul> <h3 id="2-nested-flatten">2. Nested Flatten</h3> <p><img src="https://i.loli.net/2019/12/10/B7laHcreNgYykAu.png" alt="image.png"></p> <h3 id="3-whats-api">3. What’s API</h3> <ul> <li> <p>APIs are url endpoints dedicated for programmatic requests setup by the website host.</p> </li> <li> <p><strong>An endpoint</strong> is a server route that is used to retrieve different data from the API.</p> </li> </ul> <h2 id="12-text-data">12. Text data</h2> <h3 id="1-regular-expression">1. Regular Expression</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
pat = '\[(.+)\/(.+)\/(.+):(.+):(.+):(.+) .+\]'
re.findall(pat, s)
</code></pre></div></div> <p><strong>The more specific, the better</strong></p> <h3 id="2-syntax">2. Syntax</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">[0-9]{2}</code> matches any 2-digit number.</li> <li> <code class="language-plaintext highlighter-rouge">[A-Z]{1}</code> matches any single occurrence of any upper-case letter.</li> <li> <code class="language-plaintext highlighter-rouge">[a-z]{2}</code> matches any 2 consecutive occurrences of lower-case letters.</li> <li>Certain special characters (<code class="language-plaintext highlighter-rouge">[</code>, <code class="language-plaintext highlighter-rouge">]</code>, <code class="language-plaintext highlighter-rouge">/</code>) need to be escaped with <code class="language-plaintext highlighter-rouge">\</code> </li> <li> <code class="language-plaintext highlighter-rouge">{1, 3}</code> between 1 and 3</li> <li> <code class="language-plaintext highlighter-rouge">[A-E]+</code> is just shorthand for <code class="language-plaintext highlighter-rouge">(A|B|C|D|E)(A|B|C|D|E)*</code> </li> </ul> <h3 id="3-python-re-lib">3. python re lib</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">re.search</code>: <ul> <li><code class="language-plaintext highlighter-rouge">m = re.search(r, s); m.groups</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">re.findall</code></li> <li><code class="language-plaintext highlighter-rouge">re.sub</code></li> </ul> <h3 id="4-bag-of-words">4. bag of words</h3> <ul> <li>Create an index out of <em>all</em> distinct words <ul> <li>The basis for the vector space of words.</li> </ul> </li> <li>Create vectors for each text entry by computing the counts of words in the entry.</li> <li>The dot product between two vectors is proportional to their ‘similarity’: <ul> <li> <table> <tbody> <tr> <td>This defines the <strong>cosine similarity</strong> between vectors via: $$dist(v, w) = 1 - \cos(\theta) = 1 - \frac{v \cdot w}{</td> <td>v</td> <td> </td> <td>w</td> <td>}$$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <h3 id="5-tf-idf">5. TF-IDF</h3> <ul> <li>tf(term frequency): likelihood of the term appearing in the document</li> <li>idf(inverse document frequency): <br> \(\log(\frac)\)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tfidf = pd.DataFrame([], index=sentences.index)  # dataframe of documents
for w in words.unique():
    re_pat = '\\b%s\\b' % w
    tf = sentences.str.count(re_pat) / (sentences.str.count(' ') + 1)
    idf = np.log(len(sentences) / sentences.str.contains(re_pat).sum())
    tfidf[w] = tf * idf
</code></pre></div></div> <h2 id="5-ngram">5. Ngram</h2> <ul> <li>N-gram language models capture this concept that only nearby words matter; they assume the probability a word occurs only depends on the previous $(N−1)$ words.</li> <li> <p>That is, an N-gram model says: $P(w_n|w_1,\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\ldots,w_{n-1})$.<br>For Example, the trigrams of the sentence $w$ in the example above are: <code class="language-plaintext highlighter-rouge">[('when', 'I', 'eat'), ('I', 'eat', 'pizza'), ... , ('wipe', 'off', 'the'), ('off', 'the', 'grease')]</code> \(P(w_1,w_2,w_3) = P(w_3 | w_1, w_2) = \frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}\)</p> </li> <li>terms: <ul> <li>corpus: the likelihood that a given sequence of words occur in a given “language”</li> <li>tokens: Computing the probabilities of a language model from a book requires breaking up the text of book into sequences of tokens. This process is called <code class="language-plaintext highlighter-rouge">tokenization</code>.</li> </ul> </li> </ul> <h2 id="13-text-and-features">13. Text and features</h2> <h2 id="1-nominal-feature-encoding-one-hot-encoding">1. Nominal feature encoding: One hot encoding</h2> <p>A column containing US states transforms into 50 feature columns</p> <h2 id="14-features-and-models">14. Features and models</h2> <h3 id="1-statistical-model-inference">1. Statistical Model: Inference</h3> <ul> <li>A <strong>statistical model</strong> is a quantitative relationship between properties in observed data. <h3 id="2-prediction-model-regression">2. Prediction Model: Regression</h3> </li> <li> <strong>Regression Models</strong> attempt to predict the most likely quantitative value associated to an observation (feature). <h3 id="3-prediction-model-classification">3. Prediction Model: Classification</h3> </li> <li> <strong>Classification Models</strong> attempt to predict the most likely <em>class</em> associated to an observation (feature).</li> </ul> <h2 id="15-modeling-pipeline">15. Modeling Pipeline</h2> <h3 id="1-transformer-classestransformer">1. Transformer Classes(Transformer)</h3> <ul> <li>Inputs and outputs are all numpy array</li> <li>Output data is also a numpy array</li> </ul> <h3 id="2-sklearn-model-classesestimator">2. Sklearn Model Classes(Estimator)</h3> <p><code class="language-plaintext highlighter-rouge">Sklearn</code> model classes (estimators) behave like transformers, but use outcomes (target variables) to fit and evaluate.</p> <table> <thead> <tr> <th>Property</th> <th>Example</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Initialize model parameters</td> <td><code class="language-plaintext highlighter-rouge">lr = LinearRegression()</code></td> <td>Create (empty) linear regression model</td> </tr> <tr> <td>Fit the model to the data</td> <td><code class="language-plaintext highlighter-rouge">lr.fit(data, outcomes)</code></td> <td>Determines regression coefficients</td> </tr> <tr> <td>Use model for prediction</td> <td><code class="language-plaintext highlighter-rouge">lr.predict(newdata)</code></td> <td>Use regression line make predictions</td> </tr> <tr> <td>Evaluate the model</td> <td><code class="language-plaintext highlighter-rouge">lr.score(data, outcomes)</code></td> <td>Calculate the $R^2$ of the LR model</td> </tr> <tr> <td>Access model attributes</td> <td><code class="language-plaintext highlighter-rouge">lr.coef_</code></td> <td>Access the regression coefficients</td> </tr> </tbody> </table> <p><em>Note:</em> Once <code class="language-plaintext highlighter-rouge">fit</code>, estimators are just transformers (<code class="language-plaintext highlighter-rouge">predict</code> &lt;-&gt; <code class="language-plaintext highlighter-rouge">transform</code>)</p> <p><strong>Design your own <code class="language-plaintext highlighter-rouge">transformer()</code>:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MyTrans(BaseEstimator, TransformerMixin):
    
    def __init__(self):
        
        pass

    def fit(self, X, y=None):

        return self

    def transform(self, X, y=None):
        
        return out
</code></pre></div></div> <h3 id="3-building-models-with-transformers-and-estimators">3. Building models with transformers and estimators</h3> <ol> <li>Define your transformations; models.</li> <li>Transform input data to features.</li> <li>Use (transformed) features to fit model.</li> <li>Predict outcomes from features using fit model.</li> </ol> <h3 id="4-pipeline">4. Pipeline</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Numeric columns and associated transformers
num_feat = ['total_bill', 'size']
num_transformer = Pipeline(steps=[
    ('scaler', pp.StandardScaler())
])

# Categorical columns and associated transformers
cat_feat = ['sex', 'smoker', 'day', 'time']
cat_transformer = Pipeline(steps=[
    ('intenc', pp.OrdinalEncoder()),
    ('onehot', pp.OneHotEncoder())
])

# preprocessing pipeline (put them together)
preproc = ColumnTransformer(transformers=[('num', num_transformer, num_feat), ('cat', cat_transformer, cat_feat)])

pl = Pipeline(steps=[('preprocessor', preproc), ('regressor', LinearRegression())])
</code></pre></div></div> <h2 id="16-bias-and-variance">16. Bias and Variance</h2> <p>Cross validation can reduce variance.</p> <ul> <li>Method 1: ``` <h1 id="scikit-learn-k-fold-cross-validation">scikit-learn k-fold cross-validation</h1> </li> </ul> <p>from sklearn.model_selection import KFold</p> <h1 id="data-sample">data sample</h1> <p>data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])</p> <h1 id="prepare-cross-validation">prepare cross validation</h1> <p>kfold = KFold(3, True, 1)</p> <h1 id="enumerate-splits">enumerate splits</h1> <p>for train, test in kfold.split(data): print(‘train: %s, test: %s’ % (data[train], data[test]))</p> <hr> <p>train: [0.1 0.4 0.5 0.6], test: [0.2 0.3] train: [0.2 0.3 0.4 0.6], test: [0.1 0.5] train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
- Method 2
    `scores = cross_val_score(pipeline, X_train, y_train, cv=3)`

- Method 3
  **`GridSearchCV()`**
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parameters = {
    'max_depth': [2,3,4,5,7,10,13,15,18,None], 
    'min_samples_split':[2,3,5,7,10,15,20],
    'min_samples_leaf':[2,3,5,7,10,15,20]
}
clf = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5)
clf.fit(X_train, y_train)   ``` ## 17. Examples ### 1. Regression with Multicollinearity * Linear regression with (perfectly) correlated features leads to high variance (unstable) models. * When the dataset ~1-dimensional in 3-dim space, fitting a plane is under-determined. * Use Principal Component Analysis to drop unneeded features.
</code></pre></div></div> <p><img src="https://i.loli.net/2019/12/10/D98phqI21MBVres.png" alt="multicollinearity.png"></p> <h2 id="18-model-evaluation">18. Model Evaluation</h2> <h3 id="1-binary-classification-outcome">1. Binary classification outcome</h3> <p>Accuracy treats TP the same as TN; not appropriate for class-imbalanced data(class imbalanced data is when the ratio of positive-negative outcomes is far from 1.) <br> When tackling class unbalanced condition, accuracy is not a good choice. We need the measurement to focus on more rare class.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Yue Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4FWDSZJVD"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L4FWDSZJVD");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>