<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>6. Training neural networks I | Yue Yang</title> <meta name="author" content="Yue Yang"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yy-gx.github.io//blog/2019/cs231n-6(Training-neural-networks-I)/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yue </span>Yang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Yue_Yang.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">6. Training neural networks I</h1> <p class="post-meta">September 23, 2019• Y. Yang</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/stanford-cs231n"> <i class="fas fa-hashtag fa-sm"></i> Stanford-CS231N</a>   </p> </header> <article class="post-content"> <h1 id="1-activation-funcs">1. Activation funcs</h1> <p><img src="https://i.loli.net/2019/09/30/hjYoabNvukL4spx.png" alt="image.png"></p> <h3 id="11-sigmoid">1.1 sigmoid</h3> <p>problems:</p> <ul> <li>big values neurons kill the gradients(大的x值会导致梯度为0)</li> <li>Not Zero-centered</li> <li>exp() is a bit compute expensive.</li> </ul> <h3 id="12-tanh">1.2 tanh</h3> <p>advantage:</p> <ul> <li>Zero-centered</li> </ul> <p>problem:</p> <ul> <li>big values neurons kill the gradients(大的x值会导致梯度为0)</li> </ul> <h3 id="13-relu">1.3 RELU</h3> <p>advantages:</p> <ul> <li>big values neurons don’t kill the gradients</li> <li>not compute expensive.</li> <li>Converges much faster than Sigmoid and Tanh</li> <li>More biologically plausible than sigmoid.</li> </ul> <p>problems:</p> <ul> <li>Not Zero-centered</li> <li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation(为了解决这个问题，可以加个bias=0.01)</li> </ul> <h3 id="14-leaky-relu">1.4 leaky RELU</h3> <p>advantages:</p> <ul> <li>Doesn’t kill the gradients from both sides.</li> <li>Computationally efficient.</li> <li>Converges much faster than Sigmoid and Tanh</li> <li>Will not die. 其中0.01也可以作为一个学习超参数</li> </ul> <h3 id="14-maxout">1.4 maxout</h3> <p>advantages:</p> <ul> <li>Generalizes RELU and Leaky RELU</li> <li>Doesn’t die!</li> </ul> <p>problems:</p> <ul> <li>double the number of parameters per neuron</li> </ul> <h3 id="15-elu">1.5 ELU</h3> <p>advantages:</p> <ul> <li>It has all the benefits of RELU</li> <li>Closer to zero mean outputs and adds some robustness to noise.</li> </ul> <p>problems:</p> <ul> <li>exp() is a bit compute expensive.</li> </ul> <h3 id="16-in-practice">1.6 In practice</h3> <ul> <li>Use RELU. Be careful for your learning rates.</li> <li>Try out Leaky RELU/Maxout/ELU</li> <li>Try out tanh but don’t expect much.</li> <li>Don’t use sigmoid!</li> </ul> <h1 id="2-data-preprocessing">2. Data preprocessing</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Zero centered data. (Calculate the mean for every input).
# One of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. 
X -= np.mean(X, axis = 1)

# Then apply the standard deviation. Hint: in images we don't do this.
X /= np.std(X, axis = 1)
</code></pre></div></div> <p>平均值两种计算方法，一种是减掉整个图片的平均值，一种是每个channel分别减去其平均值</p> <h1 id="3-weight-initialization">3. Weight initialization</h1> <h2 id="31-初始化为0">3.1 初始化为0</h2> <p>所有的神经元都一样了，更新的时候梯度全都一样</p> <h2 id="32-随机化">3.2 随机化</h2> <p>两种：</p> <p>第一种在大型的神经网络上会导致后面的节点出现梯度消失的问题：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W = 0.01 * np.random.rand(D, H)
# Works OK for small networks but it makes problems with deeper networks!
</code></pre></div></div> <p>第二种在大型的神经网络上会导致后面的节点出现梯度爆炸的问题：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W = 1 * np.random.rand(D, H) 
# Works OK for small networks but it makes problems with deeper networks!
</code></pre></div></div> <h2 id="33-xavier-initialization">3.3 Xavier initialization</h2> <p>这种初始化目的是为了让每一层输出的方差一样，这样可以让信息更顺畅地在网络中流畅</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W = np.random.rand(in, out) / np.sqrt(in)
</code></pre></div></div> <p>但是使用relu的时候该方法会失效的</p> <h2 id="34-he-initialization-solution-for-the-relu-issue">3.4 He initialization (Solution for the RELU issue)</h2> <p>解决Xavier的relu失效问题：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W = np.random.rand(in, out) / np.sqrt(in/2)
</code></pre></div></div> <h1 id="4-batch-normalization">4. Batch normalization</h1> <p>该技术是为了让每一层的输入都是mean = 0, var = 1</p> <p>一般该层是放在卷积之后，激活层之前的</p> <p>步骤：</p> <ol> <li>计算平均值以及方差</li> <li>减去平均值，除以根号下方差+epsilon，epsilon的目的就是为了让分母不为0</li> <li>接下来shift + scale：<code class="language-plaintext highlighter-rouge">Result = gamma * normalizedX + beta</code>。其中gamma和beta都是可学习的超参数。目的是让NN可以学习各种概率分布，因为不一定都高斯分布是最好的。这样每一层就会更加灵活。</li> </ol> <p>batch normalization的好处：</p> <ul> <li>Networks train faster.</li> <li>Allows higher learning rates.</li> <li>helps reduce the sensitivity to the initial starting weights.</li> <li>Makes more activation functions viable.</li> <li>Provides some regularization. Because we are calculating mean and variance for each batch that gives a slight regularization effect.</li> </ul> <p>卷积层后面，都是每个activation map就算一个mean和var，不是几层一块算的！</p> <p>常规的CONV和NN中batch normalization是best，但是循环的NN和rf中这还是热门的研究。</p> <h1 id="5-baby-sitting-the-learning-process">5. Baby sitting the learning process</h1> <ul> <li>Preprocessing of data.</li> <li>Choose the architecture.</li> <li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li> <li>Add regularization, the loss should go up!</li> <li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss. (You should overfit perfectly for small datasets.)</li> <li>Take your full training data, and small regularization then try some value of learning rate. <ul> <li>If loss is barely changing, then the learning rate is small.</li> <li>If you got NAN then your NN exploded and your learning rate is high.</li> <li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li> </ul> </li> <li>Do Hyperparameters optimization to get the best hyperparameters values.</li> </ul> <h1 id="6-hyperparameter-optimization">6. Hyperparameter Optimization</h1> <ul> <li>Cross validation strategy</li> <li>It‘s best to optimize in log space</li> <li>Adjust your ranges and try again.</li> <li>Its better to try random search instead of grid searches (In log space)</li> </ul> <hr> <h1 id="problems">Problems</h1> <ol> <li>怎么判断训练是否是更快了？</li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Yue Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4FWDSZJVD"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L4FWDSZJVD");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>